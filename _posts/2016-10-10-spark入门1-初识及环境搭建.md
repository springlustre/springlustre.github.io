---
layout: post
title: spark入门1-初识及环境搭建
excerpt: "spark入门1-初识及环境搭建"
categories: 环境搭建
tags: 
- 环境搭建
- spark
comments: true
---

* Table of Contents
{:toc}



## Spark介绍

### 概述
Spark 由加州大学伯克利分校 AMP 实验室 (Algorithms, Machines, and People Lab) 开发，可用来构建大型的、低延迟的数据分析应用程序。
Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以
优化迭代工作负载。

Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。与 Hadoop 不同，Spark 和 Scala 能够紧密集成，其中的 Scala 可以像操作本地集合对象一样轻松地操作分布式数据集。
尽管创建 Spark 是为了支持分布式数据集上的迭代作业，但是实际上它是对 Hadoop 的补充，可以在 Hadoo 文件系统中并行运行。通过名为 Mesos 的第三方集群框架可以支持此行为。


Spark 引进了内存集群计算的概念，可在内存集群计算中将数据集缓存在内存中，以缩短访问延迟。
Spark 还引进了弹性分布式数据集 (RDD) 的概念。RDD 是分布在一组节点中的只读对象集合。这些集合是弹性的，如果数据集一部分丢失，则可以对它们进行重建。重建部分数据集的过程依赖于容错机制。RDD 被表示为一个 Scala 对象，并且可以从文件中创建它。
Spark 中的应用程序称为驱动程序，这些驱动程序可实现在单一节点上执行的操作或在一组节点上并行执行的操作。与 Hadoop 类似，Spark 支持单节点集群或多节点集群。对于多节点操作，Spark 依赖于 Mesos 集群管理器。Mesos 为分布式应用程序的资源共享和隔离提供了一个有效平台。

### RDD介绍
RDD，全称为Resilient Distributed Datasets，是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作实现了monad模式，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作（注意，reduceByKey是action，而非transformation），以支持常见的数据运算。

RDD提供了两方面的特性persistence和patitioning，用户可以通过persist与patitionBy函数来控制RDD的这两个方面。RDD的分区特性与并行计算能力(RDD定义了parallerize函数)，使得Spark可以更好地利用可伸缩的硬件资源。若将分区与持久化二者结合起来，就能更加高效地处理海量数据。例如：

```
input.map(parseArticle _).partitionBy(partitioner).cache()
```

partitionBy函数需要接受一个Partitioner对象，如：

```
val partitioner = new HashPartitioner(sc.defaultParallelism)
```

RDD本质上是一个内存数据集，在访问RDD时，指针只会指向与操作相关的部分。例如存在一个面向列的数据结构，其中一个实现为Int的数组，另一个实现为Float的数组。如果只需要访问Int字段，RDD的指针可以只访问Int数组，避免了对整个数据结构的扫描。

RDD将操作分为两类：transformation与action。无论执行了多少次transformation操作，RDD都不会真正执行运算，只有当action操作被执行时，运算才会触发。而在RDD的内部实现机制中，底层接口则是基于迭代器的，从而使得数据访问变得更高效，也避免了大量中间结果对内存的消耗。

在实现时，RDD针对transformation操作，都提供了对应的继承自RDD的类型，例如map操作会返回MappedRDD，而flatMap则返回FlatMappedRDD。当我们执行map或flatMap操作时，不过是将当前RDD对象传递给对应的RDD对象而已.

以下面一个按 A-Z 首字母分类，查找相同首字母下不同姓名总个数的例子来看一下 RDD 是如何运行起来的。
![Elaphurus davidianus](http://images0.cnblogs.com/blog/107289/201508/111610089731588.jpg)

步骤 1 ：创建 RDD  

上面的例子除去最后一个 collect 是个动作，不会创建 RDD 之外，前面四个转换都会创建出新的 RDD 。因此第一步就是创建好所有 RDD( 内部的五项信息 ) 。

步骤 2 ：创建执行计划 

Spark 会尽可能地管道化，并基于是否要重新组织数据来划分阶段 (stage) ，例如本例中的 groupBy() 转换就会将整个执行计划划分成两阶段执行。最终会产生一个 DAG(directed acyclic graph ，有向无环图 ) 作为逻辑执行计划。
![Elaphurus davidianus](http://images0.cnblogs.com/blog/107289/201508/111610154426712.jpg)

步骤 3 ：调度任务  

将各阶段划分成不同的 任务 (task) ，每个任务都是数据和计算的合体。在进行下一阶段前，当前阶段的所有任务都要执行完成。因为下一阶段的第一个转换一定是重新组织数据的，所以必须等当前阶段所有结果数据都计算出来了才能继续。


### 懒惰计算

懒惰计算（lazy evaluation）：Spark在遇到 Transformations操作时只会记录需要这样的操作，并不会去执行，需要等到有Actions操作的时候才会真正启动计算过程进行计算。（不像Python和matlab马上执行）。



## Spark 安装

### 安装jdk和scala

下载安装JDK：https://www.oracle.com/index.html 并配置环境变量，不再详述。

下载Scala： http://www.scala-lang.org/。
解压缩：tar –zxvf scala-2.10.6.tgz。
进入sudo vim /etc/profile 在下面添加路径：
```
SPARK_HOME=/home/spark/spark-1.5.1-bin-hadoop2.6
PATH=$PATH:${SCALA_HOME}/bin
``` 
使修改生效source /etc/profile。
在命令行输入scala测试。

进入spark的bin目录
输入run-example org.apache.spark.examples.SparkPi 
出现如下结果则安装成功

![Elaphurus davidianus](../../../images/2016_10_16_1.png)


